%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[twoside]{article}

\usepackage{lipsum} % Package to generate dummy text throughout this template

\usepackage[sc]{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage[utf8]{inputenc}
\linespread{1.05} % Line spacing - Palatino needs more space between lines
\usepackage{microtype} % Slightly tweak font spacing for aesthetics
\usepackage{graphicx}

\usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt]{geometry} % Document margins
\usepackage{multicol} % Used for the two-column layout of the document
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption} % Custom captions under/above floats in tables or figures
\usepackage{booktabs} % Horizontal rules in tables
\usepackage{float} % Required for tables and figures in the multi-column environment - they need to be placed in specific locations with the [H] (e.g. \begin{table}[H])
\usepackage{hyperref} % For hyperlinks in the PDF

\usepackage{lettrine} % The lettrine is the first enlarged letter at the beginning of the text
\usepackage{paralist} % Used for the compactitem environment which makes bullet points with less space between them

\usepackage{abstract} % Allows abstract customization
\renewcommand{\abstractnamefont}{\normalfont\bfseries} % Set the "Abstract" text to bold
\renewcommand{\abstracttextfont}{\normalfont\small\itshape} % Set the abstract itself to small italic text

\usepackage{titlesec} % Allows customization of titles
\renewcommand\thesection{\Roman{section}} % Roman numerals for the sections
\renewcommand\thesubsection{\Roman{subsection}} % Roman numerals for subsections
\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{} % Change the look of the section titles
\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{} % Change the look of the section titles

\usepackage{amsmath}
\usepackage{algpseudocode}
\usepackage{algorithm}

\newcommand{\todo}[1]{\textbf{TODO(#1)}}
\renewcommand{\v}[1]{\vec{#1}}

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

% TODO: otsikosta saa 2/40 p, voi ansaita hiontaa kun työ on pidemmmällä. 

\title{\vspace{-15mm}\fontsize{24pt}{10pt}\selectfont\textbf{Predicting wine color and quality using chemical characteristics}}

\author{
\large
\textsc{A comparison of machine learning methods}\\[2mm]
\textsc{Hannu Siikonen, Mikko Perttunen}\\[2mm]
\vspace{-5mm}
}
\date{}

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Insert title

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\begin{abstract}
% TODO: myöskin 2/40 p, 100-200 sanaa

\noindent Katsoppas uudestaan. Utsoppas kaadestaan. Sas.

\end{abstract}

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\begin{multicols}{2} % Two-column layout throughout the main article text

\section{Introduction}
% 4/40 p
% Introduces the reader to the topic and the broad context within which the research fits
% Why is the task important, what question is being addressed, what is hoped to learn?
% Literature review

The goal of this study is to predict the type and the quality of a wine by making use of a given dataset.
This is realized by comparing the strengths and weaknesses of different machine learning 
and statistical modelling algorithms when applied to the present data. Optimized learning algorithms
are then revised based on the performance of different methods.
 
All possible algorithms cannot be well-equipped for these analyses. Nevertheless,
a variety of methods is used. This is important when dealing with a randomly chosen dataset, since according to
Wolpert\todo{\LaTeX-viittaus}, there is no simple way to predetermine which
learning algorithm yields the best results with the data. In other words, it can be difficult to assess
which method the inductive bias that corresponds best to the data without resorting to trial and error.
The evaluated algorithms were selected based on their diversity and using some initial hypotheses.

For the sake of variety, the performance of both regression and classification algorithms is evaluated.
However, the focus is on classification algorithms - to some extent due to it being more straightforward to compare the 
performance of classification models. Moreover, both the prediction of the type and the quality of the wine can be considered
classification problems, whereas only the quality can be studied also using regression. It is obvious that
there is some correlation between neighboring classes, that is, between wine quality values that differ by only one point. This makes
using regression a good idea, although on the other hand the integer presentation of the quality makes using regression somewhat cumbersome.

The dataset that is used to evaluate algorithms is derived from the so called \emph{Vinho verde} dataset by Cortez et al. \cite{CorCer09}.
It consists of 6000 observations, each describing an individual wine. For each one of them, 11 variables are provided in addition
to the color and quality of the wine: fixed acidity, volatile acidity, density, pH and amounts of citric acid, residual sugar, chlorides, 
free sulfur diodixe, total sulfur dioxide, sulphates and alcohol in the wine. The color of each wine is either red or white 
and the quality is an integer between 1 and 7. 5000 observations are used as training data, while the other 1000 are reserved for testing.

Wine data is in general a favored object for study in the field of machine learning. There have been various studies
on the subject, for instance those of Cortez et al. \cite{WQA}, \cite{CorCer09} 
and Appalsamy et al. \cite{Appalsami}. One motive for this are the large and growing markets of the wine industry.
Being able to predict the quality of a wine without the need for a wine taster, as well as a deeper understanding
of the properties of a good wine, induce much potential for profit. On the other hand, the problem of predicting
the quality of a wine is intriguing from a machine learning perspective. The quality is determined by human taste,
a very complex and subjective system, difficult to predict only by using a limited set of chemical properties.
One more reason to explain the popularity of wine quality predictions is the fact that there is a lot of data
on wines available on the Internet.

This task is interesting, since the prediction problem is twofold: both a clear physical property and an abstract, subjective property
are to be predicted. The former should be relatively easily predictable, assuming that a sufficient set
of chemical properties is given. The prediction of the quality, on the other hand, can be a very challenging task.
In general, these kinds of hard problems are often ones that are good for benchmarking machine learning algorithms.
In addition to that, with such difficult problems a great interest exists for a decently performant prediction algorithm,
since prioer to sufficient computing power and analysis methods they have been insolvable. Thus, this study provides a
good review of the features of modern machine learning algorithms when applied to a complex dataset.
 
\section{Methods}
% 8/40 p
% Methods used to conduct your experiment
% Concept-level: Methods/algos and modifications to these

For predicting the behavior of the wine data, many machine learning methods are equipped.
To some extent the same classification algorithms are used with minor differences for both
the analysis of the type and the analysis of the quality of wines. On top of this, to augur the 
quality, some regression algorithms are utilized. The basic properties of the used algorithms
are described in this section. The theoretical background of most of the algorithms can be found
in a basic reference, such as Ref. \cite{Alpaydin}. 

\subsection{Classification}

\subsubsection{Naive Gaussian discrimination}

While taking initial steps with a dataset, it is usually instructive to begin with a simple
learning algorithm. At the simplest, this could mean using a model that returns a constant value or
class. A slightly more complicated simple model is provided byt using only one of the eleven variables
given in the data. The initial hypothesis is that for some of the parameters, all classes are somewhat
normally distributed for this variable $x$. It follows that by calculating the mean value 
$\mu$ and variance $\sigma^2$ of each class using the training set, the probability distributions of 
each class $k$ is obtained:
\begin{equation}
 p(x|\mu_k,\sigma_k) = \frac{1}{\sqrt{2\pi} \sigma_k} e^{-\frac{(x-\mu_k)^2}{2\sigma_k^2}}.
\end{equation}
This can be used directly as a likelihood function for the different classes or a Bayesian posterior
can be taken:
\begin{equation}\label{posterior}
 p(\mu_k,\sigma_k|x) \propto p(x|\mu_k,\sigma_k) p(\mu_k,\sigma_k),
\end{equation}
where $p(\mu_k,\sigma_k) = p(\theta_k)$ is simply obtained as the fraction of the class $k$ in the 
training set. This can be transformed conveniently into a discriminant, by choosing a reference
class $j = k$:
\begin{equation}\label{naiividiskr}
 g_k(x) = \log \frac{p(\mu_k,\sigma_k|x)}{p(\mu_j,\sigma_j|x)}.
\end{equation}
The maximal $g_k(x)$ indicates the predicted class $k$. By simplifying Eq. \eqref{naiividiskr},
it is obtained
\begin{equation}\label{naiividiskr2}
 g_k(x) = \frac{(x-\mu_j)^2}{2\sigma_k^2}-\frac{(x-\mu_k)^2}{2\sigma_k^2} + 
 \log \frac{\sigma_j}{\sigma_k}.
\end{equation}


\subsubsection{Multivariate Gaussian discrimination}

The multivariate gaussian discriminant operates analogically to the one-dimensional
case, extending the discriminant function to eleven variables. It determines the class of a sample by
comparing the probabilities of it belonging to each class, when each
class is modelled using a multivariate Gaussian distribution. This can be problematic,
since many real-world variables have a clearly non-Gaussian distribution.

The Gaussian distributions are determined based on training data.
The mean vector $\v{\mu}_k$ and covariance matrix $\mathbf{\Sigma}_k$ are calculated for each class.
The probability of a sample $x$ belonging to the class can then be calculated
using the multivariate Gaussian probability density function:

\begin{equation}\label{monigauss}
 p(\v{x}|\v{\mu}_k,\mathbf{\Sigma_k}) = \frac{1}{\sqrt{2\pi} |\mathbf{\Sigma_k}|}
 e^{-\frac{1}{2}(\v{x}-\v{\mu}_k)^T \mathbf{\Sigma}_k^{-1} (\v{x}-\v{\mu}_k)}
\end{equation}

The likelihood of Eq. \eqref{monigauss} can be used directly for estimating the class of a
sample $\v{x}$, or a Bayesian posterior can be utilized as in Eq. \eqref{posterior}. 
A discriminant can be constructed parallelly to Eqs. \eqref{naiividiskr} and \eqref{naiividiskr2}:
\begin{equation}
	\begin{aligned}
	 g_k(\v{x}) = \frac{1}{2}(\v{x}-\v{\mu}_j)^T \mathbf{\Sigma}_j^{-1} (\v{x}-\v{\mu}_j) - \\
	 \frac{1}{2}(\v{x}-\v{\mu}_k)^T \mathbf{\Sigma}_k^{-1} (\v{x}-\v{\mu}_k) 
	+\log \frac{|\mathbf{\Sigma}_j|}{|\mathbf{\Sigma}_k|}.
       \end{aligned}
\end{equation}
Again, the class with the largest discriminant $g_k(\v{x})$ indicates the prediction of klass, $k$.

\subsubsection{$k$ nearest neighbours}

The $k$ nearest neighbours classifier is a non-parametric classifier. Only a
distance function between the points in the variable space is required.
The classification given by $k$ nearest neighbours for a new point $\v{x}$ is then found by simply
selecting the $k$ points closest to $x$ according to the distance function and
picking the most common class among the $k$ points. The training set is fully
stored in memory. The samples in it are used for the nearest neighbour study
of the points in the validation or the test set.

Selecting a good distance function is important, especially when many variables
are used for the prediction. The distributions of the
input variables can vary greatly. When using the Euclidean distance function,
this causes certain variables to have a greater effect on the distance between
points when compared to others. To prevent this, we used the Mahalanobis
distance function\cite[p.~88]{Alpaydin}:

\begin{equation}\label{eq:mahalanobis}
  D_M(\v{x}, \v{y}) = \sqrt{(\v{x}-\v{y})^T \Sigma^{-1} (\v{x}-\v{y})}
\end{equation}

where $\Sigma$ is the covariance matrix for the 
distribution $\v{x}$ and $\v{y}$ are taken from.
This normalizes the distances in the different dimensions of
the variable space and also takes into account unneccessary correlations.
The Mahalanobis distance is an intuitive distance measure for an arbitrarily
scaled space. It appears for instance in the exponent of the multivariate
Gaussian distribution of Eq. \eqref{monigauss}.

If the class probabilities $p_c(\v{x})$ for a point $\v{x}$ are required, these can
be calculated using the nearest neighbours of it. If the frequency of a class $c$
within the $k$ nearest neighbours is $k_c$, the probability is simply
\begin{equation}
 p(\v{x}) = \frac{k_c}{k}.
\end{equation}

\subsubsection{Linear discrimination}

The methods that rely on discrimination model directly the class discriminants instead of
the class probabilities. In linear discrimination the discriminant for a class $k$ is linear:
\begin{equation}
 g_k(\v{x}) = \v{w}_k^T v{x} + w_{k0}.
\end{equation}
The basic theory of linear discrimination is not deeper than this: the goal is to find such
coefficients $\v{w}_k$, $w_{k0}$ that the classification error is minimal. Usually this has
to be done numerically for instance with the steepest descent algorithm. There are multiple 
ways to perform the optimization. One example
is logistic discrimination, in which the class probabilities are defined by a connection to the class
probabilities. The discriminant of a class $k$ is a logarithm of the fraction of the likelihood of
this class divided by the likelihood of a reference class. This definition gives some convenient 
properties for the model from the viewpoint of numerical solutions. However, also other
approaches may be used.

\subsubsection{Quadratic discrimination}

Quadratic discrimination is otherwise similar to linear discrimination, but it equips a quadratic discriminant,
\begin{equation}
 g_k(\v{x}) = \v{x}^T \mathbf{W}_k \v{x} + \v{w}_k^T \v{x} + w_{k0}.
\end{equation}
Because of the quadratic form, the search for optimal discriminants is much more complicated than with linear
discriminants. Additionally, the additional complexity brought by the quadratic form may be unnecessary.
Nevertheless, quadratic discrimination provides a good algorithm for comparison, if an efficient algorithm
for it is available.

\subsubsection{Support vector machine}

Support vector machines are a variant of linear discrimination. If classes do not overlap, it finds
the separating hyperplanes between classes that have the largest margins. These hyperplanes correspond 
to the linear discriminants. With overlapping classes, a penalty term is added to the solution in order
to make the same approach work. The discriminants trained with the training set are used for validation
and test sets.

\subsubsection{Random forest}

Random forest \cite{Forest} is an ensemble learning method that equips an ensemble of classification trees. Each tree 
in the ensemble does classification using only a chosen number of the given variables. This could be for
instance the square root of the total amount of variables. For this kind of an ensemble of trees
the trees themselves are usually unpruned. The trees themselves are simply ordinary classification trees:
they use the given variables and split the branches until a desired amount of purity or other conditions
are fulfilled for the training set. It is utterly important that there is a sufficient amount of classification
trees in the ensemble. With a too small amount of trees sufficient statistics are not reached and the algorithm
does not perform optimally.

\subsection{Regression}

\subsubsection{Random forest, regression}

The random forest algorithm can be used also for regression. In this case, the classification trees in the 
ensemble are substituted with regression trees. A regression tree gives effectively a constant value as an output,
since a typical tree cannot easily give continuous output. When an ensemble of regression trees is considered, the 
situation is a bit more interesting. The final output is averaged over many trees and thus the output resembles
more a continuous behaviour.

\subsubsection{Linear least squares}

The linear (or ordinary) least squares method is a regression algorithm that
finds a linear fit for one variable as a function of multiple variables: $\hat{y} = \v{a}^T \v{x}+b$. 
The vector $\v{a}$ and the constant $b$ are such that in the training set $\sum (y - \hat{y})$ is 
minimized, hence the name least squares.

The problem can be conveniently presented in a matrix form. Let $\mathbf{X}$ be a matrix
with rows $[1, \v{x}^T]$, composed of all the samples in the training set. That is, the 
constant $b$ is now included in the vector for the sake of convenience. Let $\v{y}$ be
a corresponding column vector and $\v{w}$ a column vector that holds the linear coefficients and
\begin{equation}
\hat{\v{y}} = \mathbf{X} \v{w}
\end{equation}
is the least squares estimate. Now the least squares problem is equal to minimizing the
vector product $(\v{y} - \hat{\v{y}})^T(\v{y} - \hat{\v{y}})$ with respect to $\v{w}$. 

It can be briefly shown that the optimal solution is given by 
\begin{equation}
 \v{w} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T y.
\end{equation}
This is a usuful form, since it can be used for an arbitrary matrix $\mathbf{X}$. That is,
$\mathbf{X}$ may hold only the given variables in a basic form, but also for instance higher
exponents of the variables can be inserted into this matrix. Thus, it is relatively simple
to control the complexity of this model.

\subsection{PCA}

Primary Component Analysis is a useful method that can be used with both classification and regression
problems. Effectively, it reduces the amount of variables that the variable that is predicted depends
on. This can be useful, if some of the provided data is not at all useful in the analysis. On the other
hand, it gives an effective method of graphical representation of the data. If the studied variable
depends on over three dimensions, using PCA it can be projected to three or two dimensions. This is
helpful while trying to develop a concrete understanding of the data used.

PCA itself relies on studying the parameters that are responsible for most of the variance of the studied
variable. The greater the effect on variance, the more the parameter explains the behaviour of the variable.
Mathematically, PCA studies the covariance matrix of the matrix that holds the training set samples of the 
variables that are used to predict the variable of interest. The eigenvectors of the covariance matrix are
the principal components. The corresponding eigenvalues indicate the relative fraction of variance that
the principal components (eigenvectors) are responsible for. 

If PCA is done with a reduction from $N$ to
$K$ dimensions, the $K$ largest eigenvalues of the covariance matrix are chosen. The data in the training set is shifted so that
its mean value is zero in all dimensions, and then it is projected on the $K$ eigenvectors that correspond
to the chosen eigenvalues. This is the PCA-representation, which can be used for graphical purposes or
also as the input data for machine learning algorithms.

\subsection{Combining learners}

If many different learners with approximately equal performance are found, it is instructive
to combine these learners to boost performance. For this, many kinds of algorithms can be used
but in this work a simple approach is taken. It is assumed that maximally three very good
learning algorithms are chosen for the final model. With these algorithms, the probabilities
of each class are calculated for each sample in the validation or testing set. The probabilities
are denoted with $P_1$, $P_2$ and $P_3$ and they are given weights $w_1$, $w_2$ and $w_3$.
The weights are normalized so that $w_1 + w_2 + w_3 = 1$. Thus the final probability is given by
\begin{equation}
 P = w_1 ( P_1 - P_3 ) + w_2 ( P_2 - P_3 ) + P_3.
\end{equation}
Because of the normalization condition and assumption that the weights are greater than zero,  $0 \leq w_1,w_2 \leq$ and $w_1 + w_2 \leq 1$. 
An approximate solution can be found by scanning the possible values of $w_1$ and $w_2$ on a lattice with a spacing of $0.01$.
The best values are chosen by validation. In this work the goal is that no more than three learning algorithms are combined.
With only two learning algorithms, only one parameter has to be scanned over.

%\begin{figure}[H]
%\centering
%\includegraphics[width=0.5\textwidth]{learning}
%\caption{Learning is important but big processors are importanter. \cite{alpaydin2004introduction}}
%\label{fig:learning}
%\end{figure}

%------------------------------------------------

\section{Experiments}
% 8/40 p
% Method and materials (applied)
% Necessary for applying the methods to the data: pre-proc., validation of params (cross-validation)
% No results here!

\subsection{Algorithm evaluation}

Algorithms were evaluated separately on the two problems: predicting wine color and quality.
Both output variables are predicted using the 11 chemical characteristics. Color wasn't used
as a predictor for quality or vice versa.
Some algorithms were evaluated with both problems while other ones were only applied to one.
Knowledge about chemistry was not used; the problems are treated purely as machine learning
problems.

For evaluating algorithms predicting the wine color, a simple scoring method was used.
There are only two classes, so the amount of classification errors was used as a quality indicator.
For wine quality, there are seven classes, so a more complex indicator was used. \emph{ADD HERE}

\subsection{Color prediction}

\subsubsection{Naive logarithmic discriminant}

\subsubsection{Gaussian logarithmic discriminant}

\emph{MENTION PCA}

\subsubsection{Linear discriminant}

\subsubsection{Random forest}

\subsubsection{$k$-nearest neighbours}

\subsection{Quality prediction}

\subsubsection{Linear discriminant}

\subsubsection{Support vector machine}

\subsubsection{Random forest}

\subsubsection{$k$-nearest neighbours}

\subsubsection{Least squares}

% \begin{table}[H]
% \caption{My current knowledge of tables}
% \centering
% \begin{tabular}{cc}
% \textbf{Table type} & \textbf{Likely location}\\
% \midrule
% Coffee table & Living room\\
% Dining table & Dining room\\
% Bedside table & Bedroom
% \end{tabular}
% \end{table}

%------------------------------------------------

\subsection{Method validation}

Method validation is done using $k$-fold 
cross validation with $k = 10$. That is, the training set is partitioned in 10 equally sized parts. 
Validation is done ten times using each partition once as a validation set, while the other nine
sections function as training sets. The mean error of the 10 validations functions as a measure
of goodness of an algorithm. This is beneficial for choosing between different algorithms or
between parameter values with a single algorithm.

\section{Results}
% 4/80 p
% Description of results with figures and tables

Joku viileä alotustarina.

\subsection{Color prediction}

The performance of the naive Gaussian discriminant, multivariate Gauss discriminant, linear discriminant, $k$ nearest neighbors and random forest
algorithms, in addition to a final hybrid algorithm, were measured on the color prediction problem.

The evaluation dataset can be visualized by projecting it into a two-dimensional plane using principal component analysis. Looking at 
figure \ref{fig:pca_color_test} we can see that while the data is somewhat separated into two groups by PCA, there is no clear separation
between the groups. For each evaluated algorithm, the rate of succesful predictions are reported. In addition, the erroneous predictions are shown on a PCA-projected
image of the observations.

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{pca_color_test}
\caption{2d PCA projection of test observations with colours}
\label{fig:pca_color_test}
\end{figure}

Since the majority of the test observations are of white wines, a natural dummy model is to predict that all wines are white. This results in an error rate of 19.6\%, as seen
in figure \ref{fig:RES_COLOR_dummy}.

\begin{figure}[H]
 \centering
 \caption{Dummy model - All white}
 \small{Error rate: 19.6\%}
 \includegraphics[width=0.5\textwidth]{color_dummy}
 \label{fig:RES_COLOR_dummy}
\end{figure}

\subsubsection{Gaussian models}

Naiivi gauss ja monimuuttujallinen. Performanssin raportointia, kuva/taulukko? Cross-validationista voi pistää kaikki kymmenen virhettä ja keskiarvon.

\subsubsection{Well-performing models}

Lineaari-diskr, kNN, rand. forest, tulokset.

\subsubsection{Final model}

Painokerrointen valintaa ja tuloksen raportointia

\subsection{Prediction of quality}

Koodi hiottava vastaavasti kuin värissä.

%------------------------------------------------

\section{Discussion}
% 4/80 p

Pohdinnot, results:issa vain raportointi, ei pohdintaa.

%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------

% 2/80 p
\bibliographystyle{plain}
\bibliography{final_report}{}

%------------------------------------------------

\section*{Appendix A}
% 4/80 p
% koodi-otteita

%----------------------------------------------------------------------------------------

\end{multicols}

\end{document}
