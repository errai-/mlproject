%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[twoside]{article}

\usepackage{lipsum} % Package to generate dummy text throughout this template

\usepackage[sc]{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage[utf8]{inputenc}
\linespread{1.05} % Line spacing - Palatino needs more space between lines
\usepackage{microtype} % Slightly tweak font spacing for aesthetics
\usepackage{graphicx}

\usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt]{geometry} % Document margins
\usepackage{multicol} % Used for the two-column layout of the document
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption} % Custom captions under/above floats in tables or figures
\usepackage{booktabs} % Horizontal rules in tables
\usepackage{float} % Required for tables and figures in the multi-column environment - they need to be placed in specific locations with the [H] (e.g. \begin{table}[H])
\usepackage{hyperref} % For hyperlinks in the PDF

\usepackage{lettrine} % The lettrine is the first enlarged letter at the beginning of the text
\usepackage{paralist} % Used for the compactitem environment which makes bullet points with less space between them

\usepackage{abstract} % Allows abstract customization
\renewcommand{\abstractnamefont}{\normalfont\bfseries} % Set the "Abstract" text to bold
\renewcommand{\abstracttextfont}{\normalfont\small\itshape} % Set the abstract itself to small italic text

\usepackage{titlesec} % Allows customization of titles
\renewcommand\thesection{\Roman{section}} % Roman numerals for the sections
\renewcommand\thesubsection{\Roman{subsection}} % Roman numerals for subsections
\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{} % Change the look of the section titles
\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{} % Change the look of the section titles

\usepackage{amsmath}
\usepackage{algpseudocode}
\usepackage{algorithm}

\newcommand{\todo}[1]{\textbf{TODO(#1)}}
\renewcommand{\v}[1]{\vec{#1}}

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\title{\vspace{-15mm}\fontsize{24pt}{10pt}\selectfont\textbf{Predicting wine color and quality using chemical characteristics}}

\author{
\large
\textsc{A comparison of machine learning methods}\\[2mm]
\textsc{Hannu Siikonen, Mikko Perttunen}\\[2mm]
\vspace{-5mm}
}
\date{}

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Insert title

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\begin{abstract}

\noindent Katsoppas uudestaan. Utsoppas kaadestaan.

\end{abstract}

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\begin{multicols}{2} % Two-column layout throughout the main article text

\section{Introduction}

The goal of this project is to compare the strengths and weaknesses of different machine learning
and statistical modelling algorithms in a practical situation. Both regression and classification
algorithms are evaluated.

The dataset that is used to evaluate algorithms is derived from the so called ``Vinho verde'' dataset by Cortez et al\cite{CorCer09}.
The dataset consists of 6000 observations, each describing an individual wine. For each wine, 11 variables are provided:
fixed acidity, volatile acidity, amounts of citric acid, residual sugar, chlorides, free sulfur diodixe, total sulfur dioxide,
density, pH, and amount of alcohol in the wine. These variables are used to predict the wine's color and quality.
The color of each wine is either red or white and the quality is an integer between 1 and 7. 5000 observations are used as
training data, while the other 1000 are reserved for testing.

%------------------------------------------------

\section{Methods}

The following machine learning methods were used in the evaluated algorithms.
Method validation was done using $k$-fold validation with $k = 10$,
that is, 10\% of the training set was reserved for validation, and validation
was done ten times, once for each non-overlapping partitioning into training and validation sets.

\subsection{Gaussian discriminant}

The gaussian discriminant determines the class of a sample by
comparing the probabilities of it belonging to each class, when each
class is modelled using a multivariate Gaussian distribution.

The Gaussian distributions are determined based on training data.
The mean vector $\v{\mu}$ and covariance matrix $\Sigma$ are calculated for each class.
The probability of a sample $x$ belonging to the class can the be calculated
using the multivariate Gaussian probability density function:

\begin{equation}
 p(\v{x}) = \frac{1}{\sqrt{2\pi} |\Sigma|}e^{-\frac{1}{2}(\v{x}-\v{\mu})^T \Sigma^{-1} (\v{x}-\v{\mu})}
\end{equation}

The class with the highest $p(\v{x})$ is then selected. In the case of two classes,
this can be simplified by setting
$p'(\v{x}) = \log \frac{p(\v{x},\theta_1)}{p(\v{x},\theta_2)}$.
The value of $p'(\v{x})$ is then positive if $\v{x}$ is classified into class 1,
and negative if into class 2. 

\subsection{$k$ nearest neighbours}

The $k$ nearest neighbours classifier is a non-parametric classifier. Only a
distance function between points and a set of pre-classified points is required.
The classification given by $k$ nearest neighbours for a new point $x$ is then found by simply
selecting the $k$ points closest to $x$ according to the distance function and
picking the most common class among the $k$ points.

Selecting a good distance function is important. The distributions of the
input variables vary greatly. When using the Euclidean distance function,
this causes certain variables to have a greater effect on the distance between
points when compared to others. To prevent this, we used the Mahalanobis
distance function\cite[p.~88]{Alpaydin}:

\begin{equation}\label{eq:mahalanobis}
  D_M(\v{x}, \v{y}) = \sqrt{(\v{x}-\v{y})^T \Sigma^{-1} (\v{x}-\v{y})}
\end{equation}

where $\Sigma$ is the covariance matrix for the 
distribution $\v{x}$ and $\v{y}$ are taken from.
This normalizes the distance for the shape of the distribution.

\subsection{Random forest}

Klassifikaatio ja regressio.

\subsection{Linear discriminant}

\subsection{Quadratic discriminant}

\subsection{Linear least squares}

The linear (or ordinary) least squares is a regression method that attempts to
find the input variable coefficients for a linear function that predicts the desired variables.
Essentially, this means finding an approximate solution for $\v{x}$ in

\begin{equation*}
 A\v{x} = \v{b}
\end{equation*}

where $A$ contains the predictor variables and $\v{b}$ the predicted variable observations
in the training data. The learned $\v{x}$ can then be used to make predictions when encountering
new observations.

The linear least squares method finds the approximate solution by minimizing the error measure

\begin{equation}
 S(\v{x}) = \sum_{i=1}^n (\v{b} - A_i \v{x})^2
\end{equation}

where $n$ is the number of training observations. This measure is minimized by

\begin{equation}
 \v{x} = (A^T A)^{-1} A^T b.
\end{equation}

\todo{t채h채n vissiin v채h채n tarkemmin}

%\begin{figure}[H]
%\centering
%\includegraphics[width=0.5\textwidth]{learning}
%\caption{Learning is important but big processors are importanter. \cite{alpaydin2004introduction}}
%\label{fig:learning}
%\end{figure}

%------------------------------------------------

\section{Experiments}

\subsection{Algorithm evaluation}

Algorithms were evaluated separately on the two problems: predicting wine color and quality.
Both output variables are predicted using the 11 chemical characteristics. Color wasn't used
as a predictor for quality or vice versa.
Some algorithms were evaluated with both problems while other ones were only applied to one.
Knowledge about chemistry was not used; the problems are treated purely as machine learning
problems.

For evaluating algorithms predicting the wine color, a simple scoring method was used.
There are only two classes, so the amount of classification errors was used as a quality indicator.
For wine quality, there are seven classes, so a more complex indicator was used. \emph{ADD HERE}

\subsection{Color prediction}

\subsubsection{Naive logarithmic discriminant}

\subsubsection{Gaussian logarithmic discriminant}

\emph{MENTION PCA}

\subsubsection{Linear discriminant}

\subsubsection{Random forest}

\subsubsection{$k$-nearest neighbours}

\subsection{Quality prediction}

\subsubsection{Linear discriminant}

\subsubsection{Support vector machine}

\subsubsection{Random forest}

\subsubsection{$k$-nearest neighbours}

\subsubsection{Least squares}

% \begin{table}[H]
% \caption{My current knowledge of tables}
% \centering
% \begin{tabular}{cc}
% \textbf{Table type} & \textbf{Likely location}\\
% \midrule
% Coffee table & Living room\\
% Dining table & Dining room\\
% Bedside table & Bedroom
% \end{tabular}
% \end{table}

%------------------------------------------------

\section{Results}


%------------------------------------------------

\section{Discussion}


%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------

\bibliographystyle{plain}
\bibliography{final_report}{}

%------------------------------------------------

\section*{Appendix A}

%----------------------------------------------------------------------------------------

\end{multicols}

\end{document}
