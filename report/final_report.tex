%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[twoside]{article}

\usepackage{lipsum} % Package to generate dummy text throughout this template

\usepackage[sc]{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage[utf8]{inputenc}
\linespread{1.05} % Line spacing - Palatino needs more space between lines
\usepackage{microtype} % Slightly tweak font spacing for aesthetics
\usepackage{graphicx}

\usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt]{geometry} % Document margins
\usepackage{multicol} % Used for the two-column layout of the document
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption} % Custom captions under/above floats in tables or figures
\usepackage{booktabs} % Horizontal rules in tables
\usepackage{float} % Required for tables and figures in the multi-column environment - they need to be placed in specific locations with the [H] (e.g. \begin{table}[H])
\usepackage{hyperref} % For hyperlinks in the PDF

\usepackage{lettrine} % The lettrine is the first enlarged letter at the beginning of the text
\usepackage{paralist} % Used for the compactitem environment which makes bullet points with less space between them

\usepackage{abstract} % Allows abstract customization
\renewcommand{\abstractnamefont}{\normalfont\bfseries} % Set the "Abstract" text to bold
\renewcommand{\abstracttextfont}{\normalfont\small\itshape} % Set the abstract itself to small italic text

\usepackage{titlesec} % Allows customization of titles
\renewcommand\thesection{\Roman{section}} % Roman numerals for the sections
\renewcommand\thesubsection{\Roman{subsection}} % Roman numerals for subsections
\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{} % Change the look of the section titles
\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{} % Change the look of the section titles

\usepackage{amsmath}
\usepackage{algpseudocode}
\usepackage{algorithm}

\newcommand{\todo}[1]{\textbf{TODO(#1)}}
\renewcommand{\v}[1]{\vec{#1}}

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

% TODO: otsikosta saa 2/40 p, voi ansaita hiontaa kun työ on pidemmmällä. 

\title{\vspace{-15mm}\fontsize{24pt}{10pt}\selectfont\textbf{Predicting wine color and quality using chemical characteristics}}

\author{
\large
\textsc{A comparison of machine learning methods}\\[2mm]
\textsc{Hannu Siikonen, Mikko Perttunen}\\[2mm]
\vspace{-5mm}
}
\date{}

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Insert title

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\begin{abstract}
% TODO: myöskin 2/40 p, 100-200 sanaa

\noindent Katsoppas uudestaan. Utsoppas kaadestaan. Sas.

\end{abstract}

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\begin{multicols}{2} % Two-column layout throughout the main article text

\section{Introduction}
% 4/40 p

The goal of this study is to predict the type and the quality of a wine by making use of a given dataset.
This is realized by comparing the strengths and weaknesses of different machine learning 
and statistical modelling algorithms when applied to the present data. According to the performance of 
different methods, optimized learning algorithms are revised.
 
All possible algorithms cannot be reasonably equipped for these analyses. Nevertheless,
a decent variety of methods is used. For a randomly chosen dataset this is important since according to
the \emph{No Free Lunch Theorem} (Wolpert, 1995), there is no simple way to predetermine, which
learning algorithm is most compatible with the data. In other words, it can be difficult to assess
without trial, which method has an inductive bias that corresponds to the data best.
The algorithms are chosen heeding to diversity and using some initial hypotheses. 

The dataset that is used to evaluate algorithms is derived from the so called \emph{Vinho verde} dataset by Cortez et al. \cite{CorCer09}.
It consists of 6000 observations, each describing an individual wine. For each one of them, 11 variables are provided in addition
to the type and quality: fixed acidity, volatile acidity, density, pH and amounts of citric acid, residual sugar, chlorides, 
free sulfur diodixe, total sulfur dioxide, sulphates and alcohol in the wine. The type of each wine is either red or white 
and the quality is an integer between 1 and 7. 5000 observations are used as training data, while the other 1000 are reserved for testing.
 
For the sake of variety, the performance of both regression and classification algorithms is evaluated.
However, the focus is on classification algorithms - to some extent since it is more straightforward to compare the 
performance of classification models. Moreover, both the determination of the type and the quality can be considered
as classification problems, but only the quality can be studied also with regression. It is obvious that
there is some correlation between neighboring classes, that is qualities that differ by one point. This makes
regression favorable, but on the other hand the integer presentation of the quality makes regression somewhat cumbersome.

Wine data are in general a favored object for study in the field of machine learning. There have been various studies
on the subject, for instance those of Cortez et al. \cite{WQA}, \cite{CorCer09} 
and Appalsamy et al. \cite{Appalsami}. One motive for this are the large and growing markets of wine industry.
The prediction of the quality of a wine without a need for a wine taster as well as a deeper understanding of the 
properties of a good wine include much potential for profit. On the other hand predicting the quality of a wine is 
intriguing from the point of view of machine learning. The quality is determined by the human taste, which is 
very complex and may be hard to predict only by using some of the chemical properties. There is also a lot of wine data available 
on the internet, which may partially explain the popularity.

This task is interesting, since the type and the quality are to be analyzed: a physical property and an abstract property determined
by human taste. The former should be easily predictable assuming that a sufficient amount of chemical
properties is given, but the prediction of the latter can be a challenging task. In general, these kinds of
hard problems are often the ones that are good for benchmark testing of machine learning algorithms. On the other
hand, with such problems there is a large interest for decently working prediction algorithms, since prior
to sufficient computational power and methods they have been insolvable. Thus, this study makes a good review of
modern learning algorithms when applied to a complex dataset.
 
\section{Methods}
% 8/40 p

The following machine learning methods were used in the evaluated algorithms.
Method validation was done using $k$-fold validation with $k = 10$,
that is, 10\% of the training set was reserved for validation, and validation
was done ten times, once for each non-overlapping partitioning into training and validation sets.

\subsection{Gaussian discriminant}

The gaussian discriminant determines the class of a sample by
comparing the probabilities of it belonging to each class, when each
class is modelled using a multivariate Gaussian distribution.

The Gaussian distributions are determined based on training data.
The mean vector $\v{\mu}$ and covariance matrix $\Sigma$ are calculated for each class.
The probability of a sample $x$ belonging to the class can the be calculated
using the multivariate Gaussian probability density function:

\begin{equation}
 p(\v{x}) = \frac{1}{\sqrt{2\pi} |\Sigma|}e^{-\frac{1}{2}(\v{x}-\v{\mu})^T \Sigma^{-1} (\v{x}-\v{\mu})}
\end{equation}

The class with the highest $p(\v{x})$ is then selected. In the case of two classes,
this can be simplified by setting
$p'(\v{x}) = \log \frac{p(\v{x},\theta_1)}{p(\v{x},\theta_2)}$.
The value of $p'(\v{x})$ is then positive if $\v{x}$ is classified into class 1,
and negative if into class 2. 

\subsection{$k$ nearest neighbours}

The $k$ nearest neighbours classifier is a non-parametric classifier. Only a
distance function between points and a set of pre-classified points is required.
The classification given by $k$ nearest neighbours for a new point $x$ is then found by simply
selecting the $k$ points closest to $x$ according to the distance function and
picking the most common class among the $k$ points.

Selecting a good distance function is important. The distributions of the
input variables vary greatly. When using the Euclidean distance function,
this causes certain variables to have a greater effect on the distance between
points when compared to others. To prevent this, we used the Mahalanobis
distance function\cite[p.~88]{Alpaydin}:

\begin{equation}\label{eq:mahalanobis}
  D_M(\v{x}, \v{y}) = \sqrt{(\v{x}-\v{y})^T \Sigma^{-1} (\v{x}-\v{y})}
\end{equation}

where $\Sigma$ is the covariance matrix for the 
distribution $\v{x}$ and $\v{y}$ are taken from.
This normalizes the distance for the shape of the distribution.

\subsection{Random forest}

Klassifikaatio ja regressio.

\subsection{Linear discriminant}

\subsection{Quadratic discriminant}

\subsection{Linear least squares}

The linear (or ordinary) least squares is a regression method that attempts to
find the input variable coefficients for a linear function that predicts the desired variables.
Essentially, this means finding an approximate solution for $\v{x}$ in

\begin{equation*}
 A\v{x} = \v{b}
\end{equation*}

where $A$ contains the predictor and $\v{b}$ the predicted variable observations
in the training data. The learned $\v{x}$ can then be used to make predictions when encountering
new observations.

The linear least squares method finds the approximate solution by minimizing the error measure

\begin{equation}
 S(\v{x}) = \sum_{i=1}^n (\v{b} - A_i \v{x})^2
\end{equation}

where $n$ is the number of training observations. This measure is minimized by

\begin{equation}
 \v{x} = (A^T A)^{-1} A^T b.
\end{equation}

\todo{tähän vissiin vähän tarkemmin}

%\begin{figure}[H]
%\centering
%\includegraphics[width=0.5\textwidth]{learning}
%\caption{Learning is important but big processors are importanter. \cite{alpaydin2004introduction}}
%\label{fig:learning}
%\end{figure}

%------------------------------------------------

\section{Experiments}

\subsection{Algorithm evaluation}

Algorithms were evaluated separately on the two problems: predicting wine color and quality.
Both output variables are predicted using the 11 chemical characteristics. Color wasn't used
as a predictor for quality or vice versa.
Some algorithms were evaluated with both problems while other ones were only applied to one.
Knowledge about chemistry was not used; the problems are treated purely as machine learning
problems.

For evaluating algorithms predicting the wine color, a simple scoring method was used.
There are only two classes, so the amount of classification errors was used as a quality indicator.
For wine quality, there are seven classes, so a more complex indicator was used. \emph{ADD HERE}

\subsection{Color prediction}

\subsubsection{Naive logarithmic discriminant}

\subsubsection{Gaussian logarithmic discriminant}

\emph{MENTION PCA}

\subsubsection{Linear discriminant}

\subsubsection{Random forest}

\subsubsection{$k$-nearest neighbours}

\subsection{Quality prediction}

\subsubsection{Linear discriminant}

\subsubsection{Support vector machine}

\subsubsection{Random forest}

\subsubsection{$k$-nearest neighbours}

\subsubsection{Least squares}

% \begin{table}[H]
% \caption{My current knowledge of tables}
% \centering
% \begin{tabular}{cc}
% \textbf{Table type} & \textbf{Likely location}\\
% \midrule
% Coffee table & Living room\\
% Dining table & Dining room\\
% Bedside table & Bedroom
% \end{tabular}
% \end{table}

%------------------------------------------------

\section{Results}


%------------------------------------------------

\section{Discussion}


%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------

\bibliographystyle{plain}
\bibliography{final_report}{}

%------------------------------------------------

\section*{Appendix A}

%----------------------------------------------------------------------------------------

\end{multicols}

\end{document}
