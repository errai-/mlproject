%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[twoside]{article}

\usepackage{lipsum} % Package to generate dummy text throughout this template

\usepackage[sc]{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage[utf8]{inputenc}
\linespread{1.05} % Line spacing - Palatino needs more space between lines
\usepackage{microtype} % Slightly tweak font spacing for aesthetics
\usepackage{graphicx}

\usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt]{geometry} % Document margins
\usepackage{multicol} % Used for the two-column layout of the document
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption} % Custom captions under/above floats in tables or figures
\usepackage{booktabs} % Horizontal rules in tables
\usepackage{float} % Required for tables and figures in the multi-column environment - they need to be placed in specific locations with the [H] (e.g. \begin{table}[H])
\usepackage{hyperref} % For hyperlinks in the PDF

\usepackage{lettrine} % The lettrine is the first enlarged letter at the beginning of the text
\usepackage{paralist} % Used for the compactitem environment which makes bullet points with less space between them

\usepackage{abstract} % Allows abstract customization
\renewcommand{\abstractnamefont}{\normalfont\bfseries} % Set the "Abstract" text to bold
\renewcommand{\abstracttextfont}{\normalfont\small\itshape} % Set the abstract itself to small italic text

\usepackage{titlesec} % Allows customization of titles
\renewcommand\thesection{\Roman{section}} % Roman numerals for the sections
\renewcommand\thesubsection{\Roman{subsection}} % Roman numerals for subsections
\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{} % Change the look of the section titles
\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{} % Change the look of the section titles

\usepackage{amsmath}
\usepackage{algpseudocode}
\usepackage{algorithm}

\newcommand{\todo}[1]{\textbf{TODO(#1)}}
\renewcommand{\v}[1]{\vec{#1}}

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

% TODO: otsikosta saa 2/40 p, voi ansaita hiontaa kun työ on pidemmmällä. 

\title{\vspace{-15mm}\fontsize{24pt}{10pt}\selectfont\textbf{Predicting wine color and quality using chemical characteristics}}

\author{
\large
\textsc{A comparison of machine learning methods}\\[2mm]
\textsc{Hannu Siikonen, Mikko Perttunen}\\[2mm]
\vspace{-5mm}
}
\date{}

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Insert title

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\begin{abstract}
% TODO: myöskin 2/40 p, 100-200 sanaa

\noindent Katsoppas uudestaan. Utsoppas kaadestaan. Sas.

\end{abstract}

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\begin{multicols}{2} % Two-column layout throughout the main article text

\section{Introduction}
% 4/40 p
% Introduces the reader to the topic and the broad context within which the research fits
% Why is the task important, what question is being addressed, what is hoped to learn?
% Literature review

The goal of this study is to predict the type and the quality of a wine by making use of a given dataset.
This is realized by comparing the strengths and weaknesses of different machine learning 
and statistical modelling algorithms when applied to the present data. Optimized learning algorithms
are then revised based on the performance of different methods.
 
All possible algorithms cannot be well-equipped for these analyses. Nevertheless,
a variety of methods is used. This is important when dealing with a randomly chosen dataset, since according to
Wolpert\todo{\LaTeX-viittaus}, there is no simple way to predetermine which
learning algorithm yields the best results with the data. In other words, it can be difficult to assess
which method the inductive bias that corresponds best to the data without resorting to trial and error.
The evaluated algorithms were selected based on their diversity and using some initial hypotheses.

For the sake of variety, the performance of both regression and classification algorithms is evaluated.
However, the focus is on classification algorithms - to some extent due to it being more straightforward to compare the 
performance of classification models. Moreover, both the prediction of the type and the quality of the wine can be considered
classification problems, whereas only the quality can be studied also using regression. It is obvious that
there is some correlation between neighboring classes, that is, between wine quality values that differ by only one point. This makes
using regression a good idea, although on the other hand the integer presentation of the quality makes using regression somewhat cumbersome.

The dataset that is used to evaluate algorithms is derived from the so called \emph{Vinho verde} dataset by Cortez et al. \cite{CorCer09}.
It consists of 6000 observations, each describing an individual wine. For each one of them, 11 variables are provided in addition
to the color and quality of the wine: fixed acidity, volatile acidity, density, pH and amounts of citric acid, residual sugar, chlorides, 
free sulfur diodixe, total sulfur dioxide, sulphates and alcohol in the wine. The color of each wine is either red or white 
and the quality is an integer between 1 and 7. 5000 observations are used as training data, while the other 1000 are reserved for testing.

Wine data is in general a favored object for study in the field of machine learning. There have been various studies
on the subject, for instance those of Cortez et al. \cite{WQA}, \cite{CorCer09} 
and Appalsamy et al. \cite{Appalsami}. One motive for this are the large and growing markets of the wine industry.
Being able to predict the quality of a wine without the need for a wine taster, as well as a deeper understanding
of the properties of a good wine, induce much potential for profit. On the other hand, the problem of predicting
the quality of a wine is intriguing from a machine learning perspective. The quality is determined by human taste,
a very complex and subjective system, difficult to predict only by using a limited set of chemical properties.
One more reason to explain the popularity of wine quality predictions is the fact that there is a lot of data
on wines available on the Internet.

This task is interesting, since the prediction problem is twofold: both a clear physical property and an abstract, subjective property
are to be predicted. The former should be relatively easily predictable, assuming that a sufficient set
of chemical properties is given. The prediction of the quality, on the other hand, can be a very challenging task.
In general, these kinds of hard problems are often ones that are good for benchmarking machine learning algorithms.
In addition to that, with such difficult problems a great interest exists for a decently performant prediction algorithm,
since prioer to sufficient computing power and analysis methods they have been insolvable. Thus, this study provides a
good review of the features of modern machine learning algorithms when applied to a complex dataset.
 
\section{Methods}
% 8/40 p
% Methods used to conduct your experiment
% Concept-level: Methods/algos and modifications to these

For predicting the behavior of the wine data, many machine learning methods are equipped.
To some extent the same classification algorithms are used with minor differences for both
the analysis of the type and the analysis of the quality of wines. On top of this, to augur the 
quality, some regression algorithms are utilized. The basic properties of the used algorithms
are described in this section. The theoretical background of most of the algorithms can be found
in a basic reference, such as Ref. \cite{Alpaydin}. 

\subsection{Classification}

\subsubsection{Naive Gaussian discrimination}

While taking initial steps with a dataset, it is usually instructive to begin with a simple
learning algorithm. At the simplest, this could mean using a model that returns a constant value or
class. A slightly more complicated simple model is provided byt using only one of the eleven variables
given in the data. The initial hypothesis is that for some of the parameters, all classes are somewhat
normally distributed for this variable $x$. It follows that by calculating the mean value 
$\mu$ and variance $\sigma^2$ of each class using the training set, the probability distributions of 
each class $k$ is obtained:
\begin{equation}
 p(x|\mu_k,\sigma_k) = \frac{1}{\sqrt{2\pi} \sigma_k} e^{-\frac{(x-\mu_k)^2}{2\sigma_k^2}}.
\end{equation}
This can be used directly as a likelihood function for the different classes or a Bayesian posterior
can be taken:
\begin{equation}\label{posterior}
 p(\mu_k,\sigma_k|x) \propto p(x|\mu_k,\sigma_k) p(\mu_k,\sigma_k),
\end{equation}
where $p(\mu_k,\sigma_k) = p(\theta_k)$ is simply obtained as the fraction of the class $k$ in the 
training set. This can be transformed conveniently into a discriminant, by choosing a reference
class $j = k$:
\begin{equation}\label{naiividiskr}
 g_k(x) = \log \frac{p(\mu_k,\sigma_k|x)}{p(\mu_j,\sigma_j|x)}.
\end{equation}
The maximal $g_k(x)$ indicates the predicted class $k$. By simplifying Eq. \eqref{naiividiskr},
it is obtained
\begin{equation}\label{naiividiskr2}
 g_k(x) = \frac{(x-\mu_j)^2}{2\sigma_k^2}-\frac{(x-\mu_k)^2}{2\sigma_k^2} + 
 \log \frac{\sigma_j}{\sigma_k}.
\end{equation}


\subsubsection{Multivariate Gaussian discrimination}

The multivariate gaussian discriminant operates analogically to the one-dimensional
case, extending the discriminant function to eleven variables. It determines the class of a sample by
comparing the probabilities of it belonging to each class, when each
class is modelled using a multivariate Gaussian distribution. This can be problematic,
since many real-world variables have a clearly non-Gaussian distribution.

The Gaussian distributions are determined based on training data.
The mean vector $\v{\mu}_k$ and covariance matrix $\mathbf{\Sigma}_k$ are calculated for each class.
The probability of a sample $x$ belonging to the class can then be calculated
using the multivariate Gaussian probability density function:

\begin{equation}\label{monigauss}
 p(\v{x}|\v{\mu}_k,\mathbf{\Sigma_k}) = \frac{1}{\sqrt{2\pi} |\mathbf{\Sigma_k}|}
 e^{-\frac{1}{2}(\v{x}-\v{\mu}_k)^T \mathbf{\Sigma}_k^{-1} (\v{x}-\v{\mu}_k)}
\end{equation}

The likelihood of Eq. \eqref{monigauss} can be used directly for estimating the class of a
sample $\v{x}$, or a Bayesian posterior can be utilized as in Eq. \eqref{posterior}. 
A discriminant can be constructed parallelly to Eqs. \eqref{naiividiskr} and \eqref{naiividiskr2}:
\begin{equation}
	\begin{aligned}
	 g_k(\v{x}) = \frac{1}{2}(\v{x}-\v{\mu}_j)^T \mathbf{\Sigma}_j^{-1} (\v{x}-\v{\mu}_j) - \\
	 \frac{1}{2}(\v{x}-\v{\mu}_k)^T \mathbf{\Sigma}_k^{-1} (\v{x}-\v{\mu}_k) 
	+\log \frac{|\mathbf{\Sigma}_j|}{|\mathbf{\Sigma}_k|}.
       \end{aligned}
\end{equation}
Again, the class with the largest discriminant $g_k(\v{x})$ indicates the prediction of klass, $k$.

\subsubsection{$k$ nearest neighbours}

The $k$ nearest neighbours classifier is a non-parametric classifier. Only a
distance function between the points in the variable space is required.
The classification given by $k$ nearest neighbours for a new point $\v{x}$ is then found by simply
selecting the $k$ points closest to $x$ according to the distance function and
picking the most common class among the $k$ points. The training set is fully
stored in memory. The samples in it are used for the nearest neighbour study
of the points in the validation or the test set.

Selecting a good distance function is important, especially when many variables
are used for the prediction. The distributions of the
input variables can vary greatly. When using the Euclidean distance function,
this causes certain variables to have a greater effect on the distance between
points when compared to others. To prevent this, we used the Mahalanobis
distance function\cite[p.~88]{Alpaydin}:

\begin{equation}\label{eq:mahalanobis}
  D_M(\v{x}, \v{y}) = \sqrt{(\v{x}-\v{y})^T \Sigma^{-1} (\v{x}-\v{y})}
\end{equation}

where $\Sigma$ is the covariance matrix for the 
distribution $\v{x}$ and $\v{y}$ are taken from.
This normalizes the distances in the different dimensions of
the variable space and also takes into account unneccessary correlations.
The Mahalanobis distance is an intuitive distance measure for an arbitrarily
scaled space. It appears for instance in the exponent of the multivariate
Gaussian distribution of Eq. \eqref{monigauss}.

If the class probabilities $p_c(\v{x})$ for a point $\v{x}$ are required, these can
be calculated using the nearest neighbours of it. If the frequency of a class $c$
within the $k$ nearest neighbours is $k_c$, the probability is simply
\begin{equation}
 p(\v{x}) = \frac{k_c}{k}.
\end{equation}

\subsubsection{Linear discrimination}

The methods that rely on discrimination model directly the class discriminants instead of
the class probabilities. In linear discrimination the discriminant for a class $k$ is linear:
\begin{equation}
 g_k(\v{x}) = \v{w}_k^T v{x} + w_{k0}.
\end{equation}
The basic theory of linear discrimination is not deeper than this: the goal is to find such
coefficients $\v{w}_k$, $w_{k0}$ that the classification error is minimal. Usually this has
to be done numerically for instance with the steepest descent algorithm. There are multiple 
ways to perform the optimization. One example
is logistic discrimination, in which the class probabilities are defined by a connection to the class
probabilities. The discriminant of a class $k$ is a logarithm of the fraction of the likelihood of
this class divided by the likelihood of a reference class. This definition gives some convenient 
properties for the model from the viewpoint of numerical solutions. However, also other
approaches may be used.

\subsubsection{Quadratic discrimination}

Quadratic discrimination is otherwise similar to linear discrimination, but it equips a quadratic discriminant,
\begin{equation}
 g_k(\v{x}) = \v{x}^T \mathbf{W}_k \v{x} + \v{w}_k^T \v{x} + w_{k0}.
\end{equation}
Because of the quadratic form, the search for optimal discriminants is much more complicated than with linear
discriminants. Additionally, the additional complexity brought by the quadratic form may be unnecessary.
Nevertheless, quadratic discrimination provides a good algorithm for comparison, if an efficient algorithm
for it is available.

\subsubsection{Support vector machine}


\subsubsection{Random forest}

Klassifikaatio

\subsection{Regression}

\subsubsection{Random forest, regression}

Lyhyt recap, mitä eroa klassifikaatioon

\subsubsection{Linear least squares}

The linear (or ordinary) least squares method is a regression algorithm that
finds a linear fit for one variable as a function of multiple variables: $\hat{y} = \v{a}^T \v{x}+b$. 
The vector $\v{a}$ and the constant $b$ are such that in the training set $\sum (y - \hat{y})$ is 
minimized, hence the name least squares.

The problem can be conveniently presented in a matrix form. Let $\mathbf{X}$ be a matrix
with rows $[1, \v{x}^T]$, composed of all the samples in the training set. That is, the 
constant $b$ is now included in the vector for the sake of convenience. Let $\v{y}$ be
a corresponding column vector and $\v{w}$ a column vector that holds the linear coefficients and
\begin{equation}
\hat{\v{y}} = \mathbf{X} \v{w}
\end{equation}
is the least squares estimate. Now the least squares problem is equal to minimizing the
vector product $(\v{y} - \hat{\v{y}})^T(\v{y} - \hat{\v{y}})$ with respect to $\v{w}$. 

It can be briefly shown that the optimal solution is given by 
\begin{equation}
 \v{w} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T y.
\end{equation}
This is a usuful form, since it can be used for an arbitrary matrix $\mathbf{X}$. That is,
$\mathbf{X}$ may hold only the given variables in a basic form, but also for instance higher
exponents of the variables can be inserted into this matrix. Thus, it is relatively simple
to control the complexity of this model.

\subsection{Combining learners}

If many different learners with approximately equal performance are found, it is instructive
to combine these learners to boost performance. For this, many kinds of algorithms can be used
but in this work a simple approach is taken. It is assumed that maximally three very good
learning algorithms are chosen for the final model. With these algorithms, the probabilities
of each class are calculated for each sample in the validation or testing set. The probabilities
are denoted with $P_1$, $P_2$ and $P_3$ and they are given weights $w_1$, $w_2$ and $w_3$.
The weights are normalized so that $w_1 + w_2 + w_3 = 1$. Thus the final probability is given by
\begin{equation}
 P = w_1 ( P_1 - P_3 ) + w_2 ( P_2 - P_3 ) + P_3.
\end{equation}
Because of the normalization condition and assumption that the weights are greater than zero,  $0 \leq w_1,w_2 \leq$ and $w_1 + w_2 \leq 1$. 
An approximate solution can be found by scanning the possible values of $w_1$ and $w_2$ on a lattice with a spacing of $0.01$.
The best values are chosen by validation.

%\begin{figure}[H]
%\centering
%\includegraphics[width=0.5\textwidth]{learning}
%\caption{Learning is important but big processors are importanter. \cite{alpaydin2004introduction}}
%\label{fig:learning}
%\end{figure}

%------------------------------------------------

\section{Experiments}
% 8/40 p
% Method and materials (applied)
% Necessary for applying the methods to the data: pre-proc., validation of params (cross-validation)
% No results here!

\subsection{Algorithm evaluation}

Algorithms were evaluated separately on the two problems: predicting wine color and quality.
Both output variables are predicted using the 11 chemical characteristics. Color wasn't used
as a predictor for quality or vice versa.
Some algorithms were evaluated with both problems while other ones were only applied to one.
Knowledge about chemistry was not used; the problems are treated purely as machine learning
problems.

For evaluating algorithms predicting the wine color, a simple scoring method was used.
There are only two classes, so the amount of classification errors was used as a quality indicator.
For wine quality, there are seven classes, so a more complex indicator was used. \emph{ADD HERE}

\subsection{Color prediction}

\subsubsection{Naive logarithmic discriminant}

\subsubsection{Gaussian logarithmic discriminant}

\emph{MENTION PCA}

\subsubsection{Linear discriminant}

\subsubsection{Random forest}

\subsubsection{$k$-nearest neighbours}

\subsection{Quality prediction}

\subsubsection{Linear discriminant}

\subsubsection{Support vector machine}

\subsubsection{Random forest}

\subsubsection{$k$-nearest neighbours}

\subsubsection{Least squares}

% \begin{table}[H]
% \caption{My current knowledge of tables}
% \centering
% \begin{tabular}{cc}
% \textbf{Table type} & \textbf{Likely location}\\
% \midrule
% Coffee table & Living room\\
% Dining table & Dining room\\
% Bedside table & Bedroom
% \end{tabular}
% \end{table}

%------------------------------------------------

\subsection{Method validation}

Method validation is done using $k$-fold 
cross validation with $k = 10$. That is, the training set is partitioned in 10 equally sized parts. 
Validation is done ten times using each partition once as a validation set, while the other nine
sections function as training sets. The mean error of the 10 validations functions as a measure
of goodness of an algorithm. This is beneficial for choosing between different algorithms or
between parameter values with a single algorithm.

\section{Results}


%------------------------------------------------

\section{Discussion}


%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------

\bibliographystyle{plain}
\bibliography{final_report}{}

%------------------------------------------------

\section*{Appendix A}

%----------------------------------------------------------------------------------------

\end{multicols}

\end{document}
